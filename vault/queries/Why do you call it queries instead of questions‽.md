#### [[index]]
Curated RAG is a simple [[prototype]] that showcases how to use [[Obsidian]] to create [[LLM]] driven [[chatbot]] instances. The plan is to make an [[open source]] [[project]] out of it

The idea is based on an [[Iterative process]]

Check out [[Quick Start]] for information of how to use it

There is a [[python app]] running in the background accessing the md files in the [[vault]]


#### [[Quick Start]]
Install [[requirements]]

Set [[environment variables]]

run [[python app]]

open [[Obsidian]] and select the "db" folder as the [[vault]]
in the [[queries folder]] create a new note, recommended to make a copy of the [[-copy meâ€½]] note

rename the note to your [[query]]

add the tag run anywhere in the note


#### [[queries folder]]
A folder inside [[Obsidian]] and file system that contains the [[queries]]
#### [[queries]]
The input to the chatbot.

see [[questions and answers]] for why we call it queries instead of questions
#### [[questions and answers]]
Since modern LLM Chatbots does so much more than respond to questions with an answer, I don't use that nomenclature in this project.
## Answer
The term "queries" is used instead of "questions" because modern Language Model (LLM) chatbots do much more than just respond to questions with answers. The term "query" is more encompassing of the various tasks these chatbots can perform.
